{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sdv\n",
    "from sdv.evaluation.single_table import evaluate_quality\n",
    "from sdv.single_table import CTGANSynthesizer\n",
    "from sdv.single_table import TVAESynthesizer\n",
    "from sdv.metadata import SingleTableMetadata\n",
    "from sklearn.impute import SimpleImputer\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting xgboost\n",
      "  Downloading xgboost-2.0.2-py3-none-win_amd64.whl (99.8 MB)\n",
      "     --------------------------------------- 99.8/99.8 MB 12.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from xgboost) (1.10.0)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from xgboost) (1.23.5)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-2.0.2\n"
     ]
    }
   ],
   "source": [
    "! pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_PATH = \"mmo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "all_data = pd.read_excel(\"Biopak data compiled final 1-70 v2.xlsx\")\n",
    "# Extract just JVA data\n",
    "n_columns = len(all_data.columns)\n",
    "jva_column_count = 14\n",
    "data = all_data.iloc[:,n_columns-14:].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Synthetic data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove features with 0 variance (constant value features)\n",
    "def get_const_value_features_to_drop(df):\n",
    "    return [e for e in df.columns if df[e].nunique() == 1]\n",
    "\n",
    "def impute_and_remove_zero_var(features_ta_r):\n",
    "    # Performing imputation to replace any NaN value in the dataset with the median of the feature\n",
    "    imputer = SimpleImputer(strategy=\"median\")\n",
    "    features_ta_r_imputed = imputer.fit_transform(features_ta_r)\n",
    "    features_ta_r_imputed_df = pd.DataFrame(features_ta_r_imputed,\n",
    "                                            columns=features_ta_r.columns)\n",
    "\n",
    "    # Remove zero variance features\n",
    "    columns_to_remove = get_const_value_features_to_drop(features_ta_r_imputed_df)\n",
    "    features_ta_r_imputed_df.drop(columns=columns_to_remove, inplace=True)\n",
    "\n",
    "    return features_ta_r_imputed_df\n",
    "\n",
    "imputed_data = impute_and_remove_zero_var(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_tvae(features_ta_r_imputed_df, metadata):\n",
    "    # Tuning a variational autoencoder\n",
    "    tvae_scores = []\n",
    "    embedding_dims = [128, 256]\n",
    "    compress_dims = [128, 256]\n",
    "    decompress_dims = [128, 256]\n",
    "\n",
    "    for embedding_dim in embedding_dims:\n",
    "        for compress_dim in compress_dims:\n",
    "            for decompress_dim in decompress_dims:\n",
    "                # Creating a Variational Autoencoder synthesizer\n",
    "                tvae_synthesizer = TVAESynthesizer(metadata,\n",
    "                                                embedding_dim=embedding_dim,\n",
    "                                                compress_dims=(compress_dim,compress_dim),\n",
    "                                                decompress_dims=(decompress_dim,decompress_dim),\n",
    "                                                epochs=500)\n",
    "                \n",
    "                # Fitting the model\n",
    "                tvae_synthesizer.fit(features_ta_r_imputed_df)\n",
    "                \n",
    "                # Generating synthetic data\n",
    "                synthetic_data = tvae_synthesizer.sample(num_rows=200)\n",
    "\n",
    "                # Evaluating synthetic data\n",
    "                quality_report = evaluate_quality(\n",
    "                    features_ta_r_imputed_df,\n",
    "                    synthetic_data,\n",
    "                    metadata,\n",
    "                    verbose=False\n",
    "                )\n",
    "\n",
    "                tvae_scores.append((quality_report.get_score(), tvae_synthesizer))\n",
    "\n",
    "    return tvae_scores\n",
    "\n",
    "def tune_ctgan(features_ta_r_imputed_df, metadata):\n",
    "    # Tuning a ctgan\n",
    "    ctgan_scores = []\n",
    "    embedding_dims = [256, 512]\n",
    "    generator_dims = [256, 512]\n",
    "    discriminator_dims = [128, 256]\n",
    "\n",
    "    for embedding_dim in embedding_dims:\n",
    "        for generator_dim in generator_dims:\n",
    "            for discriminator_dim in discriminator_dims:\n",
    "                # Creating a ctgan synthesizer\n",
    "                ctgan_synthesizer = CTGANSynthesizer(metadata,\n",
    "                                                    embedding_dim=embedding_dim,\n",
    "                                                    generator_dim=(generator_dim,generator_dim),\n",
    "                                                    discriminator_dim=(discriminator_dim,discriminator_dim),\n",
    "                                                    epochs=500)\n",
    "                \n",
    "                # Fitting the model\n",
    "                ctgan_synthesizer.fit(features_ta_r_imputed_df)\n",
    "                \n",
    "                # Generating synthetic data\n",
    "                synthetic_data = ctgan_synthesizer.sample(num_rows=200)\n",
    "\n",
    "                # Evaluating synthetic data\n",
    "                quality_report = evaluate_quality(\n",
    "                    features_ta_r_imputed_df,\n",
    "                    synthetic_data,\n",
    "                    metadata,\n",
    "                    verbose=False\n",
    "                )\n",
    "\n",
    "                ctgan_scores.append((quality_report.get_score(), ctgan_synthesizer))\n",
    "\n",
    "    return ctgan_scores\n",
    "\n",
    "def run_augmentation_pipeline(features_ta_r_imputed_df):\n",
    "\n",
    "    # Creating metadata object to get metadata about the original dataset of extracted features\n",
    "    metadata = SingleTableMetadata()\n",
    "    metadata.detect_from_dataframe(data=features_ta_r_imputed_df)\n",
    "\n",
    "    # Get tvae and ctgan tuning results\n",
    "    tvae_scores = tune_tvae(features_ta_r_imputed_df, metadata)\n",
    "    ctgan_scores = tune_ctgan(features_ta_r_imputed_df, metadata)\n",
    "\n",
    "    # Return scores\n",
    "    return tvae_scores, ctgan_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvae_scores, ctgan_scores = run_augmentation_pipeline(imputed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TVAE-TA-R</th>\n",
       "      <th>CTGAN-TA-R</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.867345</td>\n",
       "      <td>0.764250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.859138</td>\n",
       "      <td>0.762946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.855135</td>\n",
       "      <td>0.759312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.851106</td>\n",
       "      <td>0.747622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.840747</td>\n",
       "      <td>0.711818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.839228</td>\n",
       "      <td>0.701221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.835476</td>\n",
       "      <td>0.694926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.816627</td>\n",
       "      <td>0.692241</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   TVAE-TA-R  CTGAN-TA-R\n",
       "0   0.867345    0.764250\n",
       "1   0.859138    0.762946\n",
       "2   0.855135    0.759312\n",
       "3   0.851106    0.747622\n",
       "4   0.840747    0.711818\n",
       "5   0.839228    0.701221\n",
       "6   0.835476    0.694926\n",
       "7   0.816627    0.692241"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a dataframe of tuning results for all\n",
    "results = pd.DataFrame({\n",
    "    \"TVAE-TA-R\": sorted([each[0] for each in tvae_scores], reverse=True),\n",
    "    \"CTGAN-TA-R\": sorted([each[0] for each in ctgan_scores], reverse=True),\n",
    "})\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtaining the best models for each of the six muscles\n",
    "best_tvae = sorted(tvae_scores, reverse=True)[0]\n",
    "best_ctgan = sorted(ctgan_scores, reverse=True)[0]\n",
    "\n",
    "# Save best models\n",
    "path = \"{}_best_models/\".format(MAIN_PATH)\n",
    "if not os.path.exists(path):  \n",
    "    os.makedirs(path)\n",
    "best_tvae[1].save(\n",
    "    filepath = path + \"/\" + \"tvae.pkl\"\n",
    ")\n",
    "best_ctgan[1].save(\n",
    "    filepath = path + \"/\" + \"ctgan.pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling rows:   0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling rows: 100%|██████████| 2000/2000 [00:00<00:00, 2140.76it/s]\n"
     ]
    }
   ],
   "source": [
    "# Loading the synthesizer\n",
    "path = \"{}_best_models/\".format(MAIN_PATH)\n",
    "best_synthesizer = TVAESynthesizer.load(\n",
    "    filepath = path + \"/\" + 'tvae.pkl'\n",
    ")\n",
    "\n",
    "# Generating 2000 synthetic observations using the trained model\n",
    "synthetic_data = best_synthesizer.sample(num_rows=2000, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine with original data\n",
    "full_data = pd.concat([synthetic_data, imputed_data], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data proprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(full_data)\n",
    "\n",
    "# Handle categorical variables\n",
    "df = pd.DataFrame(scaled_data, columns=full_data.columns)\n",
    "df_encoded = pd.get_dummies(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data = full_data.iloc[2000:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 14)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
