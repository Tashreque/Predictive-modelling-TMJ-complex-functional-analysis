{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sdv\n",
    "from sdv.evaluation.single_table import evaluate_quality\n",
    "from sdv.single_table import CTGANSynthesizer\n",
    "from sdv.single_table import TVAESynthesizer\n",
    "from sdv.metadata import SingleTableMetadata\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBRegressor\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import KFold, cross_validate\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variable\n",
    "CURR_ACTIVITY = \"mmo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "all_data = pd.read_excel(\"Biopak data compiled final 1-70 v2.xlsx\")\n",
    "\n",
    "# Extract just JVA data\n",
    "n_columns = len(all_data.columns)\n",
    "step = 14\n",
    "if CURR_ACTIVITY == \"map\":\n",
    "    start = n_columns - step\n",
    "elif CURR_ACTIVITY == \"mle\":\n",
    "    start = n_columns - (step*2)\n",
    "else:\n",
    "    start = n_columns - (step*3)\n",
    "data = all_data.iloc[:,start:start+step].copy()\n",
    "\n",
    "data[\"ROM Slant\"] = all_data[\"ROM Slant\"]\n",
    "data[\"Max opening\"] = all_data[\"Deviation max point during opening slant\"]\n",
    "data[\"Max closing\"] = all_data[\"Deviation max point during closing slant\"]\n",
    "data[\"Opening lateral right\"] = all_data[\"Deviation max point during opening Lateral right\"]\n",
    "data[\"Opening lateral left\"] = all_data[\"Deviation max point during opening Lateral left\"]\n",
    "data[\"Closing lateral right\"] = all_data[\"Deviation max point during closing Lateral right\"]\n",
    "data[\"Closing lateral left\"] = all_data[\"Deviation max point during closing Lateral left\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This function adds duration and intensity quotients to the existing data.\n",
    "'''\n",
    "def add_quotients(muscle, data):\n",
    "    intensity_quotients = pd.read_csv(f\"quotients/{muscle}/quotient_intensity.csv\")\n",
    "    duration_quotients = pd.read_csv(f\"quotients/{muscle}/quotient_duration.csv\")\n",
    "    temp_data_intensity = intensity_quotients.copy()\n",
    "    temp_data_duration = duration_quotients.copy()\n",
    "    temp_data_intensity.columns = [f\"{muscle}_intensity_\" + name\n",
    "                                for name in temp_data_intensity.columns]\n",
    "    temp_data_duration.columns = [f\"{muscle}_duration_\" + name\n",
    "                                for name in temp_data_duration.columns]\n",
    "    \n",
    "    indices_to_insert_nan = [1, 40, 42, 60]\n",
    "\n",
    "    # Insert NaN values without replacing existing values\n",
    "    for index in indices_to_insert_nan:\n",
    "        temp_data_intensity = pd.concat([temp_data_intensity.iloc[:index],\n",
    "                                        pd.DataFrame([[np.nan] * len(temp_data_intensity.columns)],\n",
    "                                                    columns=temp_data_intensity.columns),\n",
    "                                                    temp_data_intensity.iloc[index:]],\n",
    "                                                    ignore_index=True).copy()\n",
    "        temp_data_duration = pd.concat([temp_data_duration.iloc[:index],\n",
    "                                        pd.DataFrame([[np.nan] * len(temp_data_duration.columns)],\n",
    "                                                    columns=temp_data_duration.columns),\n",
    "                                                    temp_data_duration.iloc[index:]],\n",
    "                                                    ignore_index=True).copy()\n",
    "        \n",
    "    # Add quotients to JVA data\n",
    "    new_data = pd.concat([data, temp_data_intensity], axis=1)\n",
    "    new_data = pd.concat([new_data, temp_data_duration], axis=1)\n",
    "\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data with quotient\n",
    "new_data = add_quotients(CURR_ACTIVITY, data)\n",
    "new_data = new_data.dropna().copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['JVA MMO left integral', 'JVA MMO left Integral <300Hz',\n",
       "       'JVA MMO left Integral >300Hz', 'JVA MMO left Integral ratio',\n",
       "       'JVA MMO left peak amplitude', 'JVA MMO left peak frequency',\n",
       "       'JVA MMO left median frequency', 'JVA MMO right integral',\n",
       "       'JVA MMO right Integral <300Hz', 'JVA MMO right Integral >300Hz',\n",
       "       'JVA MMO right Integral ratio', 'JVA MMO right peak amplitude',\n",
       "       'JVA MMO right peak frequency', 'JVA MMO right median frequency',\n",
       "       'ROM Slant', 'Max opening', 'Max closing', 'Opening lateral right',\n",
       "       'Opening lateral left', 'Closing lateral right', 'Closing lateral left',\n",
       "       'mmo_intensity_ta_r', 'mmo_intensity_ta_l', 'mmo_intensity_mm_r',\n",
       "       'mmo_intensity_mm_l', 'mmo_intensity_da_r', 'mmo_intensity_da_l',\n",
       "       'mmo_duration_ta_r', 'mmo_duration_ta_l', 'mmo_duration_mm_r',\n",
       "       'mmo_duration_mm_l', 'mmo_duration_da_r', 'mmo_duration_da_l'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Synthetic data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove features with 0 variance (constant value features)\n",
    "def get_const_value_features_to_drop(df):\n",
    "    return [e for e in df.columns if df[e].nunique() == 1]\n",
    "\n",
    "def impute_and_remove_zero_var(features_ta_r):\n",
    "    # Remove zero variance features\n",
    "    columns_to_remove = get_const_value_features_to_drop(features_ta_r)\n",
    "    features_ta_r.drop(columns=columns_to_remove, inplace=True)\n",
    "\n",
    "    return features_ta_r\n",
    "\n",
    "processed_data = impute_and_remove_zero_var(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_tvae(features_ta_r_imputed_df, metadata):\n",
    "    # Tuning a variational autoencoder\n",
    "    tvae_scores = []\n",
    "    embedding_dims = [128, 256]\n",
    "    compress_dims = [128, 256]\n",
    "    decompress_dims = [128, 256]\n",
    "\n",
    "    for embedding_dim in embedding_dims:\n",
    "        for compress_dim in compress_dims:\n",
    "            for decompress_dim in decompress_dims:\n",
    "                # Creating a Variational Autoencoder synthesizer\n",
    "                tvae_synthesizer = TVAESynthesizer(metadata,\n",
    "                                                embedding_dim=embedding_dim,\n",
    "                                                compress_dims=(compress_dim,compress_dim),\n",
    "                                                decompress_dims=(decompress_dim,decompress_dim),\n",
    "                                                epochs=500)\n",
    "                \n",
    "                # Fitting the model\n",
    "                tvae_synthesizer.fit(features_ta_r_imputed_df)\n",
    "                \n",
    "                # Generating synthetic data\n",
    "                synthetic_data = tvae_synthesizer.sample(num_rows=200)\n",
    "\n",
    "                # Evaluating synthetic data\n",
    "                quality_report = evaluate_quality(\n",
    "                    features_ta_r_imputed_df,\n",
    "                    synthetic_data,\n",
    "                    metadata,\n",
    "                    verbose=False\n",
    "                )\n",
    "\n",
    "                tvae_scores.append((quality_report.get_score(), tvae_synthesizer))\n",
    "\n",
    "    return tvae_scores\n",
    "\n",
    "def tune_ctgan(features_ta_r_imputed_df, metadata):\n",
    "    # Tuning a ctgan\n",
    "    ctgan_scores = []\n",
    "    embedding_dims = [256, 512]\n",
    "    generator_dims = [256, 512]\n",
    "    discriminator_dims = [128, 256]\n",
    "\n",
    "    for embedding_dim in embedding_dims:\n",
    "        for generator_dim in generator_dims:\n",
    "            for discriminator_dim in discriminator_dims:\n",
    "                # Creating a ctgan synthesizer\n",
    "                ctgan_synthesizer = CTGANSynthesizer(metadata,\n",
    "                                                    embedding_dim=embedding_dim,\n",
    "                                                    generator_dim=(generator_dim,generator_dim),\n",
    "                                                    discriminator_dim=(discriminator_dim,discriminator_dim),\n",
    "                                                    epochs=500)\n",
    "                \n",
    "                # Fitting the model\n",
    "                ctgan_synthesizer.fit(features_ta_r_imputed_df)\n",
    "                \n",
    "                # Generating synthetic data\n",
    "                synthetic_data = ctgan_synthesizer.sample(num_rows=200)\n",
    "\n",
    "                # Evaluating synthetic data\n",
    "                quality_report = evaluate_quality(\n",
    "                    features_ta_r_imputed_df,\n",
    "                    synthetic_data,\n",
    "                    metadata,\n",
    "                    verbose=False\n",
    "                )\n",
    "\n",
    "                ctgan_scores.append((quality_report.get_score(), ctgan_synthesizer))\n",
    "\n",
    "    return ctgan_scores\n",
    "\n",
    "def run_augmentation_pipeline(features_ta_r_imputed_df):\n",
    "\n",
    "    # Creating metadata object to get metadata about the original dataset of extracted features\n",
    "    metadata = SingleTableMetadata()\n",
    "    metadata.detect_from_dataframe(data=features_ta_r_imputed_df)\n",
    "\n",
    "    # Get tvae and ctgan tuning results\n",
    "    tvae_scores = tune_tvae(features_ta_r_imputed_df, metadata)\n",
    "    ctgan_scores = tune_ctgan(features_ta_r_imputed_df, metadata)\n",
    "\n",
    "    # Return scores\n",
    "    return tvae_scores, ctgan_scores\n",
    "\n",
    "tvae_scores, ctgan_scores = run_augmentation_pipeline(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TVAE</th>\n",
       "      <th>CTGAN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.865984</td>\n",
       "      <td>0.783699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.854784</td>\n",
       "      <td>0.783497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.854280</td>\n",
       "      <td>0.773307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.851297</td>\n",
       "      <td>0.758952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.848791</td>\n",
       "      <td>0.745825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.845079</td>\n",
       "      <td>0.745177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.833094</td>\n",
       "      <td>0.744206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.831847</td>\n",
       "      <td>0.726132</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       TVAE     CTGAN\n",
       "0  0.865984  0.783699\n",
       "1  0.854784  0.783497\n",
       "2  0.854280  0.773307\n",
       "3  0.851297  0.758952\n",
       "4  0.848791  0.745825\n",
       "5  0.845079  0.745177\n",
       "6  0.833094  0.744206\n",
       "7  0.831847  0.726132"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a dataframe of tuning results for all\n",
    "results = pd.DataFrame({\n",
    "    \"TVAE\": sorted([each[0] for each in tvae_scores], reverse=True),\n",
    "    \"CTGAN\": sorted([each[0] for each in ctgan_scores], reverse=True),\n",
    "})\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtaining the best models for each of the six muscles\n",
    "best_tvae = sorted(tvae_scores, reverse=True)[0]\n",
    "best_ctgan = sorted(ctgan_scores, reverse=True)[0]\n",
    "\n",
    "# Save best models\n",
    "path = \"obj_2_best_models/\"\n",
    "if not os.path.exists(path):  \n",
    "    os.makedirs(path)\n",
    "best_tvae[1].save(\n",
    "    filepath = path + \"/\" + \"tvae.pkl\"\n",
    ")\n",
    "best_ctgan[1].save(\n",
    "    filepath = path + \"/\" + \"ctgan.pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling rows: 100%|██████████| 2000/2000 [00:02<00:00, 915.00it/s]\n"
     ]
    }
   ],
   "source": [
    "# Loading the synthesizer\n",
    "path = \"obj_2_best_models/\"\n",
    "best_synthesizer = TVAESynthesizer.load(\n",
    "    filepath = path + \"/\" + \"tvae.pkl\"\n",
    ")\n",
    "\n",
    "# Generating 2000 synthetic observations using the trained model\n",
    "synthetic_data = best_synthesizer.sample(num_rows=2000, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_to_drop_opening = []\n",
    "for i,(a,b) in enumerate(zip(synthetic_data[\"Opening lateral left\"],\n",
    "                             synthetic_data[\"Opening lateral right\"])):\n",
    "    if a > 0 and b > 0:\n",
    "        indices_to_drop_opening.append(i)\n",
    "synthetic_data.drop(indices_to_drop_opening, inplace=True)\n",
    "# Reset indices\n",
    "synthetic_data = synthetic_data.reset_index(drop=True)\n",
    "\n",
    "indices_to_drop_closing = []\n",
    "for i,(a,b) in enumerate(zip(synthetic_data[\"Closing lateral left\"],\n",
    "                             synthetic_data[\"Closing lateral right\"])):\n",
    "    if a > 0 and b > 0:\n",
    "        indices_to_drop_closing.append(i)\n",
    "synthetic_data.drop(indices_to_drop_closing, inplace=True)\n",
    "# Reset indices\n",
    "synthetic_data = synthetic_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine with original data\n",
    "full_data = pd.concat([synthetic_data, processed_data], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add final target variables of deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data[\"Lateral deviation opening\"] = np.abs(\n",
    "    full_data[\"Opening lateral left\"] - full_data[\"Opening lateral right\"])\n",
    "full_data[\"Lateral deviation closing\"] = np.abs(\n",
    "    full_data[\"Closing lateral left\"] - full_data[\"Closing lateral right\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixing names of columns\n",
    "column_names = full_data.columns\n",
    "new_column_names = [\n",
    "    each.replace(\"<\", \"less than \").replace(\">\", \"greater than \").replace(\",\", \" \")\n",
    "    for each in column_names]\n",
    "full_data.columns = new_column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>JVA MMO left integral</th>\n",
       "      <th>JVA MMO left Integral less than 300Hz</th>\n",
       "      <th>JVA MMO left Integral greater than 300Hz</th>\n",
       "      <th>JVA MMO left Integral ratio</th>\n",
       "      <th>JVA MMO left peak amplitude</th>\n",
       "      <th>JVA MMO left peak frequency</th>\n",
       "      <th>JVA MMO left median frequency</th>\n",
       "      <th>JVA MMO right integral</th>\n",
       "      <th>JVA MMO right Integral less than 300Hz</th>\n",
       "      <th>JVA MMO right Integral greater than 300Hz</th>\n",
       "      <th>...</th>\n",
       "      <th>mmo_intensity_da_r</th>\n",
       "      <th>mmo_intensity_da_l</th>\n",
       "      <th>mmo_duration_ta_r</th>\n",
       "      <th>mmo_duration_ta_l</th>\n",
       "      <th>mmo_duration_mm_r</th>\n",
       "      <th>mmo_duration_mm_l</th>\n",
       "      <th>mmo_duration_da_r</th>\n",
       "      <th>mmo_duration_da_l</th>\n",
       "      <th>Lateral deviation opening</th>\n",
       "      <th>Lateral deviation closing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11.9</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.08</td>\n",
       "      <td>2.0</td>\n",
       "      <td>27</td>\n",
       "      <td>64</td>\n",
       "      <td>14.8</td>\n",
       "      <td>13.9</td>\n",
       "      <td>1.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>52.69</td>\n",
       "      <td>12.80</td>\n",
       "      <td>96.89</td>\n",
       "      <td>56.62</td>\n",
       "      <td>36.86</td>\n",
       "      <td>40.65</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.8</td>\n",
       "      <td>14.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.9</td>\n",
       "      <td>81</td>\n",
       "      <td>79</td>\n",
       "      <td>13.0</td>\n",
       "      <td>7.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.13</td>\n",
       "      <td>23.54</td>\n",
       "      <td>39.19</td>\n",
       "      <td>123.74</td>\n",
       "      <td>50.61</td>\n",
       "      <td>26.15</td>\n",
       "      <td>54.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.2</td>\n",
       "      <td>8.9</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.7</td>\n",
       "      <td>77</td>\n",
       "      <td>80</td>\n",
       "      <td>10.5</td>\n",
       "      <td>14.5</td>\n",
       "      <td>0.9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.08</td>\n",
       "      <td>8.68</td>\n",
       "      <td>27.54</td>\n",
       "      <td>95.01</td>\n",
       "      <td>57.42</td>\n",
       "      <td>49.50</td>\n",
       "      <td>85.00</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17.3</td>\n",
       "      <td>10.9</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.8</td>\n",
       "      <td>61</td>\n",
       "      <td>72</td>\n",
       "      <td>16.1</td>\n",
       "      <td>20.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.07</td>\n",
       "      <td>39.08</td>\n",
       "      <td>15.65</td>\n",
       "      <td>114.11</td>\n",
       "      <td>70.23</td>\n",
       "      <td>25.75</td>\n",
       "      <td>107.22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21.9</td>\n",
       "      <td>23.2</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.03</td>\n",
       "      <td>2.1</td>\n",
       "      <td>54</td>\n",
       "      <td>63</td>\n",
       "      <td>20.3</td>\n",
       "      <td>59.0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.12</td>\n",
       "      <td>38.08</td>\n",
       "      <td>124.60</td>\n",
       "      <td>53.96</td>\n",
       "      <td>36.20</td>\n",
       "      <td>50.15</td>\n",
       "      <td>21.97</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   JVA MMO left integral  JVA MMO left Integral less than 300Hz  \\\n",
       "0                   11.9                                    7.0   \n",
       "1                   15.8                                   14.2   \n",
       "2                   10.2                                    8.9   \n",
       "3                   17.3                                   10.9   \n",
       "4                   21.9                                   23.2   \n",
       "\n",
       "   JVA MMO left Integral greater than 300Hz  JVA MMO left Integral ratio  \\\n",
       "0                                       0.6                         0.08   \n",
       "1                                       1.3                         0.09   \n",
       "2                                       0.4                         0.08   \n",
       "3                                       1.4                         0.11   \n",
       "4                                       1.5                         0.03   \n",
       "\n",
       "   JVA MMO left peak amplitude  JVA MMO left peak frequency  \\\n",
       "0                          2.0                           27   \n",
       "1                          0.9                           81   \n",
       "2                          0.7                           77   \n",
       "3                          0.8                           61   \n",
       "4                          2.1                           54   \n",
       "\n",
       "   JVA MMO left median frequency  JVA MMO right integral  \\\n",
       "0                             64                    14.8   \n",
       "1                             79                    13.0   \n",
       "2                             80                    10.5   \n",
       "3                             72                    16.1   \n",
       "4                             63                    20.3   \n",
       "\n",
       "   JVA MMO right Integral less than 300Hz  \\\n",
       "0                                    13.9   \n",
       "1                                     7.2   \n",
       "2                                    14.5   \n",
       "3                                    20.5   \n",
       "4                                    59.0   \n",
       "\n",
       "   JVA MMO right Integral greater than 300Hz  ...  mmo_intensity_da_r  \\\n",
       "0                                        1.5  ...                0.10   \n",
       "1                                        1.0  ...                0.09   \n",
       "2                                        0.9  ...                0.09   \n",
       "3                                        1.5  ...                0.08   \n",
       "4                                        2.3  ...                0.18   \n",
       "\n",
       "   mmo_intensity_da_l  mmo_duration_ta_r  mmo_duration_ta_l  \\\n",
       "0                0.10              52.69              12.80   \n",
       "1                0.13              23.54              39.19   \n",
       "2                0.08               8.68              27.54   \n",
       "3                0.07              39.08              15.65   \n",
       "4                0.12              38.08             124.60   \n",
       "\n",
       "   mmo_duration_mm_r  mmo_duration_mm_l  mmo_duration_da_r  mmo_duration_da_l  \\\n",
       "0              96.89              56.62              36.86              40.65   \n",
       "1             123.74              50.61              26.15              54.93   \n",
       "2              95.01              57.42              49.50              85.00   \n",
       "3             114.11              70.23              25.75             107.22   \n",
       "4              53.96              36.20              50.15              21.97   \n",
       "\n",
       "   Lateral deviation opening  Lateral deviation closing  \n",
       "0                        0.4                        1.6  \n",
       "1                        0.0                        0.0  \n",
       "2                        2.5                        3.1  \n",
       "3                        0.0                        0.1  \n",
       "4                        3.0                        3.8  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add first categorical column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_first_categorical(col_opening, col_closing):\n",
    "    cat = []\n",
    "    for val_1, val_2 in zip(col_opening,\n",
    "                            col_closing):\n",
    "        if val_1 > 0 and val_2 > 0:\n",
    "            cat.append(\"both\")\n",
    "        elif val_1 == 0 and val_2 == 0:\n",
    "            cat.append(\"no\")\n",
    "        else:\n",
    "            if val_1 > 0:\n",
    "                cat.append(\"opening\")\n",
    "            else:\n",
    "                cat.append(\"closing\")\n",
    "    return cat\n",
    "\n",
    "full_data[\"Deviation scenario\"] = create_first_categorical(full_data[\"Max opening\"],\n",
    "                                                           full_data[\"Max closing\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add second categorical column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_second_categorical(column, comparison_column):\n",
    "    cat = []\n",
    "    for val_1, val_2 in zip(column,\n",
    "                            comparison_column):\n",
    "        if val_1 == 0:\n",
    "            cat.append(\"no\")\n",
    "        else:\n",
    "            step = val_2 / 3\n",
    "            if val_1 < step:\n",
    "                cat.append(\"first\")\n",
    "            elif val_1 >= step and val_1 < step * 2:\n",
    "                cat.append(\"second\")\n",
    "            elif val_1 >= step * 2:\n",
    "                cat.append(\"third\")\n",
    "    return cat\n",
    "\n",
    "full_data[\"Deviation opening\"] = create_second_categorical(full_data[\"Max opening\"],\n",
    "                                                           full_data[\"ROM Slant\"])\n",
    "full_data[\"Deviation closing\"] = create_second_categorical(full_data[\"Max closing\"],\n",
    "                                                           full_data[\"ROM Slant\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add third categorical column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_third_categorical(col_left, col_right):\n",
    "    cat = []\n",
    "    for val_1, val_2 in zip(col_left,\n",
    "                            col_right):\n",
    "        if val_1 == 0 and val_2 == 0:\n",
    "            cat.append(\"no\")\n",
    "        elif val_1 > 0 and val_2 > 0:\n",
    "            cat.append(\"both\")\n",
    "        else:\n",
    "            if val_1 > 0:\n",
    "                cat.append(\"left\")\n",
    "            else:\n",
    "                cat.append(\"right\")\n",
    "    return cat\n",
    "\n",
    "full_data[\"Deviation direction opening\"] = create_third_categorical(full_data[\"Opening lateral left\"],\n",
    "                                                                    full_data[\"Opening lateral right\"])\n",
    "full_data[\"Deviation direction closing\"] = create_third_categorical(full_data[\"Closing lateral left\"],\n",
    "                                                                    full_data[\"Closing lateral right\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['JVA MMO left integral', 'JVA MMO left Integral less than 300Hz',\n",
       "       'JVA MMO left Integral greater than 300Hz',\n",
       "       'JVA MMO left Integral ratio', 'JVA MMO left peak amplitude',\n",
       "       'JVA MMO left peak frequency', 'JVA MMO left median frequency',\n",
       "       'JVA MMO right integral', 'JVA MMO right Integral less than 300Hz',\n",
       "       'JVA MMO right Integral greater than 300Hz',\n",
       "       'JVA MMO right Integral ratio', 'JVA MMO right peak amplitude',\n",
       "       'JVA MMO right peak frequency', 'JVA MMO right median frequency',\n",
       "       'ROM Slant', 'Max opening', 'Max closing', 'Opening lateral right',\n",
       "       'Opening lateral left', 'Closing lateral right', 'Closing lateral left',\n",
       "       'mmo_intensity_ta_r', 'mmo_intensity_ta_l', 'mmo_intensity_mm_r',\n",
       "       'mmo_intensity_mm_l', 'mmo_intensity_da_r', 'mmo_intensity_da_l',\n",
       "       'mmo_duration_ta_r', 'mmo_duration_ta_l', 'mmo_duration_mm_r',\n",
       "       'mmo_duration_mm_l', 'mmo_duration_da_r', 'mmo_duration_da_l',\n",
       "       'Lateral deviation opening', 'Lateral deviation closing',\n",
       "       'Deviation scenario', 'Deviation opening', 'Deviation closing',\n",
       "       'Deviation direction opening', 'Deviation direction closing'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Common functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_splits(X, y, type = \"classification\"):\n",
    "    # Separating original observations\n",
    "    # original_x = X.iloc[2000:,:]\n",
    "    # original_y = y.iloc[2000:]\n",
    "    # X_temp = X.iloc[:2000,:]\n",
    "    # y_temp = y.iloc[:2000]\n",
    "\n",
    "    # print(y_temp.value_counts())\n",
    "    # # Creating train test splits\n",
    "    # if type == \"classification\":\n",
    "    #     X_train, X_test, y_train, y_test = train_test_split(\n",
    "    #         X_temp, y_temp, train_size=0.8,\n",
    "    #         random_state=42, stratify=y_temp\n",
    "    #     )\n",
    "    # else:\n",
    "    #     X_train, X_test, y_train, y_test = train_test_split(\n",
    "    #         X_temp, y_temp, train_size=0.8,\n",
    "    #         random_state=42\n",
    "    #     )\n",
    "    # X_test = pd.concat([X_test, original_x]).copy()\n",
    "    # y_test = pd.concat([y_test, original_y]).copy()\n",
    "    # print(y_test.value_counts())\n",
    "    total_synthetic = len(X) - 66\n",
    "    X_train = X.iloc[:total_synthetic,:].copy()\n",
    "    y_train = y.iloc[:total_synthetic].copy()\n",
    "    X_test = X.iloc[total_synthetic:,].copy()\n",
    "    y_test = y.iloc[total_synthetic:].copy()\n",
    "    return (X_train, X_test, \n",
    "            y_train, y_test)\n",
    "\n",
    "\"\"\"\n",
    "The following function was taken from\n",
    "https://xgboost.readthedocs.io/en/stable/python/sklearn_estimator.html\n",
    "\"\"\"\n",
    "def fit_and_score(estimator, X_train,\n",
    "                  X_test, y_train,\n",
    "                  y_test, type=\"classification\"):\n",
    "    \"\"\"Fit the estimator on the train set and score it on both sets\"\"\"\n",
    "    estimator.fit(X_train, y_train,\n",
    "                  eval_set=[(X_test, y_test)], verbose=False)\n",
    "    predictions_train = estimator.predict(X_train)\n",
    "    predictions_test = estimator.predict(X_test)\n",
    "    if type == \"classification\":\n",
    "        train_score = estimator.score(X_train, y_train)\n",
    "        test_score = estimator.score(X_test, y_test)\n",
    "    else:\n",
    "        train_score = np.sqrt(mean_squared_error(y_train,\n",
    "                                                predictions_train))\n",
    "        test_score = np.sqrt(mean_squared_error(y_test,\n",
    "                                                predictions_test))\n",
    "    return estimator, train_score, test_score\n",
    "\n",
    "def do_logistic_r(X_train, y_train):\n",
    "    scores_lr = []\n",
    "    # Create a CV instance\n",
    "    cv_lr = StratifiedKFold(n_splits=5, shuffle=True,\n",
    "                            random_state=42)\n",
    "    for train, val in cv_lr.split(X_train, y_train):\n",
    "        X_train_temp = X_train[train]\n",
    "        X_val_temp = X_train[val]\n",
    "        y_train_temp = y_train[train]\n",
    "        y_val_temp = y_train[val]\n",
    "\n",
    "        # Define model\n",
    "        lr_model = LogisticRegression(multi_class='multinomial',\n",
    "                                      max_iter=1000)\n",
    "        lr_model.fit(X_train_temp, y_train_temp)\n",
    "\n",
    "        # Predictions\n",
    "        predictions_train = lr_model.predict(X_train_temp)\n",
    "        predictions_val = lr_model.predict(X_val_temp)\n",
    "\n",
    "        train_acc = accuracy_score(y_train_temp,\n",
    "                                   predictions_train)\n",
    "        val_acc = accuracy_score(y_val_temp,\n",
    "                                  predictions_val)\n",
    "        scores_lr.append((val_acc, train_acc))\n",
    "\n",
    "    # Compute average scores\n",
    "    avg_val_acc_lr = np.sum([each[0]\n",
    "                            for each in scores_lr]) / len(scores_lr)\n",
    "    avg_train_acc_lr = np.sum([each[1]\n",
    "                              for each in scores_lr]) / len(scores_lr)\n",
    "    return avg_train_acc_lr, avg_val_acc_lr\n",
    "\n",
    "def do_linear_r(X_train, y_train):\n",
    "    scores_lr = []\n",
    "    # Create a CV instance\n",
    "    cv_lr = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    for train, val in cv_lr.split(X_train, y_train):\n",
    "        X_train_temp = X_train[train]\n",
    "        X_val_temp = X_train[val]\n",
    "        y_train_temp = y_train[train]\n",
    "        y_val_temp = y_train[val]\n",
    "\n",
    "        # Define model\n",
    "        lr_model = LinearRegression()\n",
    "        lr_model.fit(X_train_temp, y_train_temp)\n",
    "\n",
    "        # Predictions\n",
    "        predictions_train = lr_model.predict(X_train_temp)\n",
    "        predictions_val = lr_model.predict(X_val_temp)\n",
    "\n",
    "        train_rmse = np.sqrt(mean_squared_error(y_train_temp,\n",
    "                                                predictions_train))\n",
    "        val_rmse = np.sqrt(mean_squared_error(y_val_temp,\n",
    "                                            predictions_val))\n",
    "        scores_lr.append((val_rmse, train_rmse))\n",
    "\n",
    "    # Compute average scores\n",
    "    avg_val_rmse_lr = np.sum([each[0]\n",
    "                            for each in scores_lr]) / len(scores_lr)\n",
    "    avg_train_rmse_lr = np.sum([each[1]\n",
    "                                for each in scores_lr]) / len(scores_lr)\n",
    "    \n",
    "    return avg_train_rmse_lr, avg_val_rmse_lr\n",
    "\n",
    "def do_xgb_classifier(X_train, y_train):\n",
    "    # Hyperparameter tuning XGBoost\n",
    "    # Parameters to tune in the grid\n",
    "    params_to_test = {\n",
    "        'max_depth': range(3,10,2),\n",
    "        'min_child_weight': range(1,6,2),\n",
    "        'n_estimators': [100, 500, 1000],\n",
    "        'learning_rate': [0.1, 0.01, 0.001]\n",
    "    }\n",
    "\n",
    "    # Performing gridSearch manually\n",
    "    best_xgb_scores = None\n",
    "    best_xgb_model = None\n",
    "    curr_val_score = -1\n",
    "    for depth in params_to_test['max_depth']:\n",
    "        for weight in params_to_test['min_child_weight']:\n",
    "            for lr in params_to_test['learning_rate']:\n",
    "                for estimator_count in params_to_test['n_estimators']:\n",
    "                    # Create a CV instance\n",
    "                    cv = StratifiedKFold(n_splits=5, shuffle=True,\n",
    "                                         random_state=42)\n",
    "\n",
    "                    # XGBoost model\n",
    "                    xgb_model = XGBClassifier(n_estimators=estimator_count, max_depth=depth,\n",
    "                                              min_child_weight=weight, early_stopping_rounds=10,\n",
    "                                              learning_rate=lr)\n",
    "\n",
    "                    # Add cross validation results\n",
    "                    total_train_score = 0\n",
    "                    total_val_score = 0\n",
    "                    for train, val in cv.split(X_train, y_train):\n",
    "                        X_train_temp = X_train[train]\n",
    "                        X_val_temp = X_train[val]\n",
    "                        y_train_temp = y_train[train]\n",
    "                        y_val_temp = y_train[val]\n",
    "\n",
    "                        est, train_score, test_score = fit_and_score(\n",
    "                            clone(xgb_model), X_train_temp,\n",
    "                            X_val_temp, y_train_temp,\n",
    "                            y_val_temp\n",
    "                        )\n",
    "                        total_train_score += train_score\n",
    "                        total_val_score += test_score\n",
    "\n",
    "                        avg_val_score = total_val_score / 5\n",
    "                        if avg_val_score > curr_val_score:\n",
    "                            curr_val_score = avg_val_score\n",
    "                            best_xgb_scores = (total_train_score / 5, avg_val_score)\n",
    "                            best_xgb_model = est\n",
    "    return best_xgb_scores, best_xgb_model\n",
    "\n",
    "def do_xgb_regressor(X_train, y_train):\n",
    "    # Hyperparameter tuning XGBoost\n",
    "    # Parameters to tune in the grid\n",
    "    params_to_test = {\n",
    "        'max_depth': range(3,10,2),\n",
    "        'min_child_weight': range(1,6,2),\n",
    "        'n_estimators': [100, 500, 1000],\n",
    "        'learning_rate': [0.1, 0.01, 0.001]\n",
    "    }\n",
    "\n",
    "    # Performing gridSearch manually\n",
    "    best_xgb_scores = None\n",
    "    best_xgb_model = None\n",
    "    curr_val_score = float('inf')\n",
    "    for depth in params_to_test['max_depth']:\n",
    "        for weight in params_to_test['min_child_weight']:\n",
    "            for lr in params_to_test['learning_rate']:\n",
    "                for estimator_count in params_to_test['n_estimators']:\n",
    "                    # Create a CV instance\n",
    "                    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "                    # XGBoost model\n",
    "                    xgb_model = XGBRegressor(n_estimators=estimator_count, max_depth=depth,\n",
    "                                             min_child_weight=weight, early_stopping_rounds=10,\n",
    "                                             learning_rate=lr)\n",
    "\n",
    "                    # Add cross validation results\n",
    "                    total_train_score = 0\n",
    "                    total_val_score = 0\n",
    "                    for train, val in cv.split(X_train, y_train):\n",
    "                        X_train_temp = X_train[train]\n",
    "                        X_val_temp = X_train[val]\n",
    "                        y_train_temp = y_train[train]\n",
    "                        y_val_temp = y_train[val]\n",
    "\n",
    "                        est, train_score, test_score = fit_and_score(\n",
    "                            clone(xgb_model), X_train_temp,\n",
    "                            X_val_temp, y_train_temp,\n",
    "                            y_val_temp, type=\"regression\"\n",
    "                        )\n",
    "                        total_train_score += train_score\n",
    "                        total_val_score += test_score\n",
    "\n",
    "                        avg_val_score = total_val_score / 5\n",
    "                        if avg_val_score < curr_val_score:\n",
    "                            curr_val_score = avg_val_score\n",
    "                            best_xgb_scores = (total_train_score / 5, avg_val_score)\n",
    "                            best_xgb_model = est\n",
    "    return best_xgb_scores, best_xgb_model\n",
    "\n",
    "def do_rf_regressor(X_train, y_train):\n",
    "    params_to_test = {\n",
    "        'max_depth': range(3,10,2),\n",
    "        'n_estimators': [100, 500, 1000],\n",
    "        'max_features': [1.0, 'sqrt', 'log2']\n",
    "    }\n",
    "    rf_grid = GridSearchCV(RandomForestRegressor(random_state=42),\n",
    "                           params_to_test, cv=5, scoring='neg_mean_squared_error',\n",
    "                           return_train_score=True)\n",
    "\n",
    "    rf_grid.fit(X_train, y_train)\n",
    "    rf_train_score = np.sqrt(\n",
    "        -rf_grid.cv_results_['mean_train_score'][rf_grid.best_index_])\n",
    "    rf_val_score = np.sqrt(-rf_grid.best_score_)\n",
    "    \n",
    "    return rf_train_score, rf_val_score, rf_grid.best_estimator_\n",
    "\n",
    "def do_rf_classifier(X_train, y_train):\n",
    "    params_to_test = {\n",
    "        'max_depth': range(3,10,2),\n",
    "        'n_estimators': [100, 500, 1000],\n",
    "        'max_features': [1.0, 'sqrt', 'log2']\n",
    "    }\n",
    "    rf_grid = GridSearchCV(RandomForestClassifier(random_state=42),\n",
    "                           params_to_test, cv=5,\n",
    "                           return_train_score=True)\n",
    "\n",
    "    rf_grid.fit(X_train, y_train)\n",
    "    rf_train_score = rf_grid.cv_results_['mean_train_score'][rf_grid.best_index_]\n",
    "    rf_val_score = rf_grid.best_score_\n",
    "    \n",
    "    return rf_train_score, rf_val_score, rf_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form data\n",
    "first_data = full_data.copy()\n",
    "first_data = full_data.drop(columns=['Max opening', 'Max closing',\n",
    "                                     'Opening lateral right', 'Opening lateral left',\n",
    "                                     'Closing lateral right', 'Closing lateral left',\n",
    "                                     'Deviation opening', 'Deviation closing',\n",
    "                                     'Deviation direction opening', \n",
    "                                     'Deviation direction closing',\n",
    "                                     'Lateral deviation opening',\n",
    "                                     'Lateral deviation closing']).copy()\n",
    "\n",
    "X_a = first_data.drop(columns=[\"Deviation scenario\"]).copy()\n",
    "y_a_org = first_data[\"Deviation scenario\"].copy()\n",
    "\n",
    "# Get splits\n",
    "X_train_a, X_test_a, y_train_a, y_test_a = get_splits(\n",
    "    X_a, y_a_org\n",
    ")\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train_a_scaled = scaler.fit_transform(X_train_a)\n",
    "X_test_a_scaled = scaler.transform(X_test_a)\n",
    "\n",
    "# Encode y\n",
    "encoder = LabelEncoder()\n",
    "y_train_a = encoder.fit_transform(y_train_a).copy()\n",
    "y_test_a = encoder.transform(y_test_a).copy()\n",
    "\n",
    "# Fit models\n",
    "log_reg_results_a = do_logistic_r(X_train_a_scaled, y_train_a)\n",
    "xgb_classifier_results_a = do_xgb_classifier(X_train_a_scaled,\n",
    "                                             y_train_a)\n",
    "rf_classifier_results_a = do_rf_classifier(X_train_a_scaled,\n",
    "                                           y_train_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [0, 303]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\Tash_files\\For TMD Diagnostics [Task 3]\\Code\\tmd_diagnostics_obj2_1.ipynb Cell 29\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Tash_files/For%20TMD%20Diagnostics%20%5BTask%203%5D/Code/tmd_diagnostics_obj2_1.ipynb#X46sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m best_model_a \u001b[39m=\u001b[39m xgb_classifier_results_a[\u001b[39m1\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Tash_files/For%20TMD%20Diagnostics%20%5BTask%203%5D/Code/tmd_diagnostics_obj2_1.ipynb#X46sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m test_predictions_a \u001b[39m=\u001b[39m best_model_a\u001b[39m.\u001b[39mpredict(X_test_a_scaled)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Tash_files/For%20TMD%20Diagnostics%20%5BTask%203%5D/Code/tmd_diagnostics_obj2_1.ipynb#X46sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m test_accuracy_a \u001b[39m=\u001b[39m accuracy_score(y_test_a, test_predictions_a)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Tash_files/For%20TMD%20Diagnostics%20%5BTask%203%5D/Code/tmd_diagnostics_obj2_1.ipynb#X46sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m test_proba_a \u001b[39m=\u001b[39m best_model_a\u001b[39m.\u001b[39mpredict_proba(X_test_a_scaled)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Tash_files/For%20TMD%20Diagnostics%20%5BTask%203%5D/Code/tmd_diagnostics_obj2_1.ipynb#X46sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m test_auc_a \u001b[39m=\u001b[39m roc_auc_score(y_test_a, test_proba_a,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Tash_files/For%20TMD%20Diagnostics%20%5BTask%203%5D/Code/tmd_diagnostics_obj2_1.ipynb#X46sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m                            multi_class\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39movr\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:192\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    187\u001b[0m validate_parameter_constraints(\n\u001b[0;32m    188\u001b[0m     parameter_constraints, params, caller_name\u001b[39m=\u001b[39mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\n\u001b[0;32m    189\u001b[0m )\n\u001b[0;32m    191\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 192\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    193\u001b[0m \u001b[39mexcept\u001b[39;00m InvalidParameterError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    194\u001b[0m     \u001b[39m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    195\u001b[0m     \u001b[39m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    196\u001b[0m     \u001b[39m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    197\u001b[0m     \u001b[39m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    198\u001b[0m     msg \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\n\u001b[0;32m    199\u001b[0m         \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+ must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    200\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    201\u001b[0m         \u001b[39mstr\u001b[39m(e),\n\u001b[0;32m    202\u001b[0m     )\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:221\u001b[0m, in \u001b[0;36maccuracy_score\u001b[1;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[39m\"\"\"Accuracy classification score.\u001b[39;00m\n\u001b[0;32m    156\u001b[0m \n\u001b[0;32m    157\u001b[0m \u001b[39mIn multilabel classification, this function computes subset accuracy:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[39m0.5\u001b[39;00m\n\u001b[0;32m    218\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    220\u001b[0m \u001b[39m# Compute accuracy for each possible representation\u001b[39;00m\n\u001b[1;32m--> 221\u001b[0m y_type, y_true, y_pred \u001b[39m=\u001b[39m _check_targets(y_true, y_pred)\n\u001b[0;32m    222\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[0;32m    223\u001b[0m \u001b[39mif\u001b[39;00m y_type\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39mmultilabel\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:86\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_targets\u001b[39m(y_true, y_pred):\n\u001b[0;32m     60\u001b[0m     \u001b[39m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \n\u001b[0;32m     62\u001b[0m \u001b[39m    This converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[39m    y_pred : array or indicator matrix\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 86\u001b[0m     check_consistent_length(y_true, y_pred)\n\u001b[0;32m     87\u001b[0m     type_true \u001b[39m=\u001b[39m type_of_target(y_true, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my_true\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     88\u001b[0m     type_pred \u001b[39m=\u001b[39m type_of_target(y_pred, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my_pred\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:397\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    395\u001b[0m uniques \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(lengths)\n\u001b[0;32m    396\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(uniques) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> 397\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    398\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    399\u001b[0m         \u001b[39m%\u001b[39m [\u001b[39mint\u001b[39m(l) \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m lengths]\n\u001b[0;32m    400\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [0, 303]"
     ]
    }
   ],
   "source": [
    "val_results_a = pd.DataFrame({\n",
    "    \"Multinomial Logistic Regression\": [log_reg_results_a[0],\n",
    "                                        log_reg_results_a[1]],\n",
    "    \"XGBoost Classifier\": [xgb_classifier_results_a[0][0],\n",
    "                           xgb_classifier_results_a[0][1]],\n",
    "    \"Random Forest Classifier\": [rf_classifier_results_a[0],\n",
    "                                 rf_classifier_results_a[1]]\n",
    "})\n",
    "val_results_a.index = [\"Train Accuracy\", \"Validation Accuracy\"]\n",
    "val_results_a.to_csv(\"obj2_results/val_results_1.csv\")\n",
    "\n",
    "# Test results based on best model (XGBoost)\n",
    "best_model_a = xgb_classifier_results_a[1]\n",
    "test_predictions_a = best_model_a.predict(X_test_a_scaled)\n",
    "test_accuracy_a = accuracy_score(y_test_a, test_predictions_a)\n",
    "test_proba_a = best_model_a.predict_proba(X_test_a_scaled)\n",
    "test_auc_a = roc_auc_score(y_test_a, test_proba_a,\n",
    "                           multi_class='ovr')\n",
    "\n",
    "test_results_a = pd.DataFrame({\n",
    "    \"Test Accuracy XGBoost\": [test_accuracy_a],\n",
    "})\n",
    "test_results_a.to_csv(\"obj2_results/test_results_1.csv\",\n",
    "                      index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Tash_files\\For TMD Diagnostics [Task 3]\\Code\\tmd_diagnostics_obj2_1.ipynb Cell 32\u001b[0m line \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Tash_files/For%20TMD%20Diagnostics%20%5BTask%203%5D/Code/tmd_diagnostics_obj2_1.ipynb#X40sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m     val_results_b\u001b[39m.\u001b[39mto_csv(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mobj2_results/val_results_2_\u001b[39m\u001b[39m{\u001b[39;00mtarget\u001b[39m}\u001b[39;00m\u001b[39m.csv\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Tash_files/For%20TMD%20Diagnostics%20%5BTask%203%5D/Code/tmd_diagnostics_obj2_1.ipynb#X40sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m (log_reg_results_b,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Tash_files/For%20TMD%20Diagnostics%20%5BTask%203%5D/Code/tmd_diagnostics_obj2_1.ipynb#X40sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m             xgb_classifier_results_b,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Tash_files/For%20TMD%20Diagnostics%20%5BTask%203%5D/Code/tmd_diagnostics_obj2_1.ipynb#X40sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m             rf_classifier_results_b,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Tash_files/For%20TMD%20Diagnostics%20%5BTask%203%5D/Code/tmd_diagnostics_obj2_1.ipynb#X40sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m             X_test_b_final, y_test_b)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Tash_files/For%20TMD%20Diagnostics%20%5BTask%203%5D/Code/tmd_diagnostics_obj2_1.ipynb#X40sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m val_results_b_opening \u001b[39m=\u001b[39m second_analysis()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Tash_files/For%20TMD%20Diagnostics%20%5BTask%203%5D/Code/tmd_diagnostics_obj2_1.ipynb#X40sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m val_results_b_closing \u001b[39m=\u001b[39m second_analysis(\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Tash_files/For%20TMD%20Diagnostics%20%5BTask%203%5D/Code/tmd_diagnostics_obj2_1.ipynb#X40sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m     target\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDeviation closing\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32md:\\Tash_files\\For TMD Diagnostics [Task 3]\\Code\\tmd_diagnostics_obj2_1.ipynb Cell 32\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Tash_files/For%20TMD%20Diagnostics%20%5BTask%203%5D/Code/tmd_diagnostics_obj2_1.ipynb#X40sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39m# Fit models\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Tash_files/For%20TMD%20Diagnostics%20%5BTask%203%5D/Code/tmd_diagnostics_obj2_1.ipynb#X40sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m log_reg_results_b \u001b[39m=\u001b[39m do_logistic_r(X_train_b_final, y_train_b)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Tash_files/For%20TMD%20Diagnostics%20%5BTask%203%5D/Code/tmd_diagnostics_obj2_1.ipynb#X40sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m xgb_classifier_results_b \u001b[39m=\u001b[39m do_xgb_classifier(X_train_b_final,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Tash_files/For%20TMD%20Diagnostics%20%5BTask%203%5D/Code/tmd_diagnostics_obj2_1.ipynb#X40sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m                                              y_train_b)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Tash_files/For%20TMD%20Diagnostics%20%5BTask%203%5D/Code/tmd_diagnostics_obj2_1.ipynb#X40sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m rf_classifier_results_b \u001b[39m=\u001b[39m do_rf_classifier(X_train_b_final,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Tash_files/For%20TMD%20Diagnostics%20%5BTask%203%5D/Code/tmd_diagnostics_obj2_1.ipynb#X40sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m                                            y_train_b)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Tash_files/For%20TMD%20Diagnostics%20%5BTask%203%5D/Code/tmd_diagnostics_obj2_1.ipynb#X40sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m val_results_b \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame({\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Tash_files/For%20TMD%20Diagnostics%20%5BTask%203%5D/Code/tmd_diagnostics_obj2_1.ipynb#X40sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mMultinomial Logistic Regression\u001b[39m\u001b[39m\"\u001b[39m: [log_reg_results_b[\u001b[39m0\u001b[39m],\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Tash_files/For%20TMD%20Diagnostics%20%5BTask%203%5D/Code/tmd_diagnostics_obj2_1.ipynb#X40sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m                                         log_reg_results_b[\u001b[39m1\u001b[39m]],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Tash_files/For%20TMD%20Diagnostics%20%5BTask%203%5D/Code/tmd_diagnostics_obj2_1.ipynb#X40sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m                                  rf_classifier_results_b[\u001b[39m1\u001b[39m]]\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Tash_files/For%20TMD%20Diagnostics%20%5BTask%203%5D/Code/tmd_diagnostics_obj2_1.ipynb#X40sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m })\n",
      "\u001b[1;32md:\\Tash_files\\For TMD Diagnostics [Task 3]\\Code\\tmd_diagnostics_obj2_1.ipynb Cell 32\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Tash_files/For%20TMD%20Diagnostics%20%5BTask%203%5D/Code/tmd_diagnostics_obj2_1.ipynb#X40sZmlsZQ%3D%3D?line=150'>151</a>\u001b[0m y_train_temp \u001b[39m=\u001b[39m y_train[train]\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Tash_files/For%20TMD%20Diagnostics%20%5BTask%203%5D/Code/tmd_diagnostics_obj2_1.ipynb#X40sZmlsZQ%3D%3D?line=151'>152</a>\u001b[0m y_val_temp \u001b[39m=\u001b[39m y_train[val]\n\u001b[1;32m--> <a href='vscode-notebook-cell:/d%3A/Tash_files/For%20TMD%20Diagnostics%20%5BTask%203%5D/Code/tmd_diagnostics_obj2_1.ipynb#X40sZmlsZQ%3D%3D?line=153'>154</a>\u001b[0m est, train_score, test_score \u001b[39m=\u001b[39m fit_and_score(\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Tash_files/For%20TMD%20Diagnostics%20%5BTask%203%5D/Code/tmd_diagnostics_obj2_1.ipynb#X40sZmlsZQ%3D%3D?line=154'>155</a>\u001b[0m     clone(xgb_model), X_train_temp,\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Tash_files/For%20TMD%20Diagnostics%20%5BTask%203%5D/Code/tmd_diagnostics_obj2_1.ipynb#X40sZmlsZQ%3D%3D?line=155'>156</a>\u001b[0m     X_val_temp, y_train_temp,\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Tash_files/For%20TMD%20Diagnostics%20%5BTask%203%5D/Code/tmd_diagnostics_obj2_1.ipynb#X40sZmlsZQ%3D%3D?line=156'>157</a>\u001b[0m     y_val_temp\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Tash_files/For%20TMD%20Diagnostics%20%5BTask%203%5D/Code/tmd_diagnostics_obj2_1.ipynb#X40sZmlsZQ%3D%3D?line=157'>158</a>\u001b[0m )\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Tash_files/For%20TMD%20Diagnostics%20%5BTask%203%5D/Code/tmd_diagnostics_obj2_1.ipynb#X40sZmlsZQ%3D%3D?line=158'>159</a>\u001b[0m total_train_score \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m train_score\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Tash_files/For%20TMD%20Diagnostics%20%5BTask%203%5D/Code/tmd_diagnostics_obj2_1.ipynb#X40sZmlsZQ%3D%3D?line=159'>160</a>\u001b[0m total_val_score \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m test_score\n",
      "\u001b[1;32md:\\Tash_files\\For TMD Diagnostics [Task 3]\\Code\\tmd_diagnostics_obj2_1.ipynb Cell 32\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Tash_files/For%20TMD%20Diagnostics%20%5BTask%203%5D/Code/tmd_diagnostics_obj2_1.ipynb#X40sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit_and_score\u001b[39m(estimator, X_train,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Tash_files/For%20TMD%20Diagnostics%20%5BTask%203%5D/Code/tmd_diagnostics_obj2_1.ipynb#X40sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m                   X_test, y_train,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Tash_files/For%20TMD%20Diagnostics%20%5BTask%203%5D/Code/tmd_diagnostics_obj2_1.ipynb#X40sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m                   y_test, \u001b[39mtype\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mclassification\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Tash_files/For%20TMD%20Diagnostics%20%5BTask%203%5D/Code/tmd_diagnostics_obj2_1.ipynb#X40sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     \u001b[39m\"\"\"Fit the estimator on the train set and score it on both sets\"\"\"\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Tash_files/For%20TMD%20Diagnostics%20%5BTask%203%5D/Code/tmd_diagnostics_obj2_1.ipynb#X40sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     estimator\u001b[39m.\u001b[39;49mfit(X_train, y_train,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Tash_files/For%20TMD%20Diagnostics%20%5BTask%203%5D/Code/tmd_diagnostics_obj2_1.ipynb#X40sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m                   eval_set\u001b[39m=\u001b[39;49m[(X_test, y_test)], verbose\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Tash_files/For%20TMD%20Diagnostics%20%5BTask%203%5D/Code/tmd_diagnostics_obj2_1.ipynb#X40sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     predictions_train \u001b[39m=\u001b[39m estimator\u001b[39m.\u001b[39mpredict(X_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Tash_files/For%20TMD%20Diagnostics%20%5BTask%203%5D/Code/tmd_diagnostics_obj2_1.ipynb#X40sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     predictions_test \u001b[39m=\u001b[39m estimator\u001b[39m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py:729\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    727\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[0;32m    728\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[1;32m--> 729\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\sklearn.py:1515\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[0;32m   1487\u001b[0m (\n\u001b[0;32m   1488\u001b[0m     model,\n\u001b[0;32m   1489\u001b[0m     metric,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1494\u001b[0m     xgb_model, eval_metric, params, early_stopping_rounds, callbacks\n\u001b[0;32m   1495\u001b[0m )\n\u001b[0;32m   1496\u001b[0m train_dmatrix, evals \u001b[39m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[0;32m   1497\u001b[0m     missing\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmissing,\n\u001b[0;32m   1498\u001b[0m     X\u001b[39m=\u001b[39mX,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1512\u001b[0m     feature_types\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_types,\n\u001b[0;32m   1513\u001b[0m )\n\u001b[1;32m-> 1515\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_Booster \u001b[39m=\u001b[39m train(\n\u001b[0;32m   1516\u001b[0m     params,\n\u001b[0;32m   1517\u001b[0m     train_dmatrix,\n\u001b[0;32m   1518\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_num_boosting_rounds(),\n\u001b[0;32m   1519\u001b[0m     evals\u001b[39m=\u001b[39;49mevals,\n\u001b[0;32m   1520\u001b[0m     early_stopping_rounds\u001b[39m=\u001b[39;49mearly_stopping_rounds,\n\u001b[0;32m   1521\u001b[0m     evals_result\u001b[39m=\u001b[39;49mevals_result,\n\u001b[0;32m   1522\u001b[0m     obj\u001b[39m=\u001b[39;49mobj,\n\u001b[0;32m   1523\u001b[0m     custom_metric\u001b[39m=\u001b[39;49mmetric,\n\u001b[0;32m   1524\u001b[0m     verbose_eval\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m   1525\u001b[0m     xgb_model\u001b[39m=\u001b[39;49mmodel,\n\u001b[0;32m   1526\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m   1527\u001b[0m )\n\u001b[0;32m   1529\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m callable(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjective):\n\u001b[0;32m   1530\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjective \u001b[39m=\u001b[39m params[\u001b[39m\"\u001b[39m\u001b[39mobjective\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py:729\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    727\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[0;32m    728\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[1;32m--> 729\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\training.py:182\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[0;32m    180\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    181\u001b[0m     bst\u001b[39m.\u001b[39mupdate(dtrain, i, obj)\n\u001b[1;32m--> 182\u001b[0m     \u001b[39mif\u001b[39;00m cb_container\u001b[39m.\u001b[39;49mafter_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    183\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    185\u001b[0m bst \u001b[39m=\u001b[39m cb_container\u001b[39m.\u001b[39mafter_training(bst)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\callback.py:241\u001b[0m, in \u001b[0;36mCallbackContainer.after_iteration\u001b[1;34m(self, model, epoch, dtrain, evals)\u001b[0m\n\u001b[0;32m    239\u001b[0m     metric_score \u001b[39m=\u001b[39m _parse_eval_str(score)\n\u001b[0;32m    240\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_history(metric_score, epoch)\n\u001b[1;32m--> 241\u001b[0m ret \u001b[39m=\u001b[39m \u001b[39many\u001b[39;49m(c\u001b[39m.\u001b[39;49mafter_iteration(model, epoch, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhistory) \u001b[39mfor\u001b[39;49;00m c \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcallbacks)\n\u001b[0;32m    242\u001b[0m \u001b[39mreturn\u001b[39;00m ret\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\callback.py:241\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    239\u001b[0m     metric_score \u001b[39m=\u001b[39m _parse_eval_str(score)\n\u001b[0;32m    240\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_history(metric_score, epoch)\n\u001b[1;32m--> 241\u001b[0m ret \u001b[39m=\u001b[39m \u001b[39many\u001b[39m(c\u001b[39m.\u001b[39;49mafter_iteration(model, epoch, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhistory) \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks)\n\u001b[0;32m    242\u001b[0m \u001b[39mreturn\u001b[39;00m ret\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\callback.py:454\u001b[0m, in \u001b[0;36mEarlyStopping.after_iteration\u001b[1;34m(self, model, epoch, evals_log)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[39m# The latest score\u001b[39;00m\n\u001b[0;32m    453\u001b[0m score \u001b[39m=\u001b[39m data_log[metric_name][\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m--> 454\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_rounds(score, data_name, metric_name, model, epoch)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\callback.py:412\u001b[0m, in \u001b[0;36mEarlyStopping._update_rounds\u001b[1;34m(self, score, name, metric, model, epoch)\u001b[0m\n\u001b[0;32m    410\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_scores[name][metric]\u001b[39m.\u001b[39mappend(score)\n\u001b[0;32m    411\u001b[0m     record \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstopping_history[name][metric][\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m--> 412\u001b[0m     model\u001b[39m.\u001b[39;49mset_attr(best_score\u001b[39m=\u001b[39;49m\u001b[39mstr\u001b[39;49m(record), best_iteration\u001b[39m=\u001b[39;49m\u001b[39mstr\u001b[39;49m(epoch))\n\u001b[0;32m    413\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_rounds \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m  \u001b[39m# reset\u001b[39;00m\n\u001b[0;32m    415\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_rounds \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrounds:\n\u001b[0;32m    416\u001b[0m     \u001b[39m# Should stop\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py:1938\u001b[0m, in \u001b[0;36mBooster.set_attr\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m   1936\u001b[0m \u001b[39mif\u001b[39;00m value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1937\u001b[0m     c_value \u001b[39m=\u001b[39m c_str(\u001b[39mstr\u001b[39m(value))\n\u001b[1;32m-> 1938\u001b[0m _check_call(_LIB\u001b[39m.\u001b[39;49mXGBoosterSetAttr(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle, c_str(key), c_value))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def second_analysis(target = \"Deviation opening\"):\n",
    "    # Form data\n",
    "    second_data = full_data.copy()\n",
    "    second_data = full_data.drop(columns=['Max opening', 'Max closing',\n",
    "                                         'Opening lateral right', 'Opening lateral left',\n",
    "                                         'Closing lateral right', 'Closing lateral left',\n",
    "                                         'Deviation direction opening', \n",
    "                                         'Deviation direction closing',\n",
    "                                         'Lateral deviation opening',\n",
    "                                         'Lateral deviation closing']).copy()\n",
    "\n",
    "    X_b = second_data.drop(columns=[\"Deviation opening\",\n",
    "                                    \"Deviation closing\"]).copy()\n",
    "    y_b_org = second_data[target].copy()\n",
    "\n",
    "    # Get splits\n",
    "    X_train_b, X_test_b, y_train_b, y_test_b = get_splits(\n",
    "        X_b, y_b_org\n",
    "    )\n",
    "\n",
    "    # Scale and create dummies\n",
    "    scaler = StandardScaler()\n",
    "    X_train_b_num = X_train_b.drop(columns=[\"Deviation scenario\"]).copy()\n",
    "    X_train_b_cat = X_train_b[\"Deviation scenario\"].copy()\n",
    "    X_test_b_num = X_test_b.drop(columns=[\"Deviation scenario\"]).copy()\n",
    "    X_test_b_cat = X_test_b[\"Deviation scenario\"].copy()\n",
    "\n",
    "    X_train_b_num_scaled = scaler.fit_transform(X_train_b_num)\n",
    "    X_test_b_num_scaled = scaler.transform(X_test_b_num)\n",
    "    X_train_b_cat_encoded = pd.get_dummies(X_train_b_cat,\n",
    "                                        dtype=int)\n",
    "    X_test_b_cat_encoded = pd.get_dummies(X_test_b_cat,\n",
    "                                        dtype=int)\n",
    "    X_train_b_final = np.concatenate([X_train_b_num_scaled,\n",
    "                                     X_train_b_cat_encoded.values],\n",
    "                                     axis=1)\n",
    "    X_test_b_final = np.concatenate([X_test_b_num_scaled,\n",
    "                                     X_test_b_cat_encoded],\n",
    "                                     axis=1)\n",
    "\n",
    "    # Encode y\n",
    "    encoder = LabelEncoder()\n",
    "    y_train_b = encoder.fit_transform(y_train_b).copy()\n",
    "    y_test_b = encoder.transform(y_test_b).copy()\n",
    "\n",
    "    # Fit models\n",
    "    log_reg_results_b = do_logistic_r(X_train_b_final, y_train_b)\n",
    "    xgb_classifier_results_b = do_xgb_classifier(X_train_b_final,\n",
    "                                                 y_train_b)\n",
    "    rf_classifier_results_b = do_rf_classifier(X_train_b_final,\n",
    "                                               y_train_b)\n",
    "    \n",
    "    val_results_b = pd.DataFrame({\n",
    "        \"Multinomial Logistic Regression\": [log_reg_results_b[0],\n",
    "                                            log_reg_results_b[1]],\n",
    "        \"XGBoost Classifier\": [xgb_classifier_results_b[0][0],\n",
    "                                xgb_classifier_results_b[0][1]],\n",
    "        \"Random Forest Classifier\": [rf_classifier_results_b[0],\n",
    "                                     rf_classifier_results_b[1]]\n",
    "    })\n",
    "    val_results_b.index = [\"Train Accuracy\", \"Validation Accuracy\"]\n",
    "    val_results_b.to_csv(f\"obj2_results/val_results_2_{target}.csv\")\n",
    "    \n",
    "    return (log_reg_results_b,\n",
    "            xgb_classifier_results_b,\n",
    "            rf_classifier_results_b,\n",
    "            X_test_b_final, y_test_b)\n",
    "\n",
    "val_results_b_opening = second_analysis()\n",
    "val_results_b_closing = second_analysis(\n",
    "    target=\"Deviation closing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test results based on best model (XGBoost)\n",
    "best_model_b_opening = val_results_b_opening[1][1]\n",
    "test_predictions_b_opening = best_model_b_opening.predict(\n",
    "    val_results_b_opening[3])\n",
    "test_accuracy_b_opening = accuracy_score(val_results_b_opening[4],\n",
    "                                         test_predictions_b_opening)\n",
    "\n",
    "test_results_b_opening = pd.DataFrame({\n",
    "    \"Test Accuracy XGBoost\": [test_accuracy_b_opening],\n",
    "})\n",
    "test_results_b_opening.to_csv(\"obj2_results/test_results_2_deviation_opening.csv\",\n",
    "                              index=False)\n",
    "\n",
    "# Test results based on best model (XGBoost)\n",
    "best_model_b_closing = val_results_b_closing[1][1]\n",
    "test_predictions_b_closing = best_model_b_closing.predict(\n",
    "    val_results_b_closing[3])\n",
    "test_accuracy_b_closing = accuracy_score(val_results_b_closing[4],\n",
    "                                         test_predictions_b_closing)\n",
    "\n",
    "test_results_b_closing = pd.DataFrame({\n",
    "    \"Test Accuracy XGBoost\": [test_accuracy_b_closing],\n",
    "})\n",
    "test_results_b_closing.to_csv(\"obj2_results/test_results_2_deviation_closing.csv\",\n",
    "                              index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right    838\n",
      "no       624\n",
      "left      53\n",
      "Name: Deviation direction opening, dtype: int64\n",
      "right    168\n",
      "no       125\n",
      "left      10\n",
      "Name: Deviation direction opening, dtype: int64\n",
      "right    1013\n",
      "no        260\n",
      "left      242\n",
      "Name: Deviation direction closing, dtype: int64\n",
      "right    203\n",
      "no        52\n",
      "left      48\n",
      "Name: Deviation direction closing, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def third_analysis(target = \"Deviation direction opening\"):\n",
    "    # Form data\n",
    "    second_data = full_data.copy()\n",
    "    second_data = full_data.drop(columns=['Max opening', 'Max closing',\n",
    "                                         'Opening lateral right', 'Opening lateral left',\n",
    "                                         'Closing lateral right', 'Closing lateral left',\n",
    "                                         'Lateral deviation opening',\n",
    "                                         'Lateral deviation closing']).copy()\n",
    "\n",
    "    X_b = second_data.drop(columns=[\"Deviation direction opening\",\n",
    "                                    \"Deviation direction closing\"]).copy()\n",
    "    y_b_org = second_data[target].copy()\n",
    "\n",
    "    # Get splits\n",
    "    X_train_b, X_test_b, y_train_b, y_test_b = get_splits(\n",
    "        X_b, y_b_org\n",
    "    )\n",
    "\n",
    "    # Scale and create dummies\n",
    "    scaler = StandardScaler()\n",
    "    X_train_b_num = X_train_b.drop(columns=[\"Deviation scenario\",\n",
    "                                            \"Deviation opening\",\n",
    "                                            \"Deviation closing\"]).copy()\n",
    "    X_train_b_cat = X_train_b[[\"Deviation scenario\",\n",
    "                               \"Deviation opening\",\n",
    "                               \"Deviation closing\"]].copy()\n",
    "    X_test_b_num = X_test_b.drop(columns=[\"Deviation scenario\",\n",
    "                                          \"Deviation opening\",\n",
    "                                          \"Deviation closing\"]).copy()\n",
    "    X_test_b_cat = X_test_b[[\"Deviation scenario\",\n",
    "                             \"Deviation opening\",\n",
    "                             \"Deviation closing\"]].copy()\n",
    "\n",
    "    X_train_b_num_scaled = scaler.fit_transform(X_train_b_num)\n",
    "    X_test_b_num_scaled = scaler.transform(X_test_b_num)\n",
    "    X_train_b_cat_encoded = pd.get_dummies(X_train_b_cat,\n",
    "                                        dtype=int)\n",
    "    X_test_b_cat_encoded = pd.get_dummies(X_test_b_cat,\n",
    "                                        dtype=int)\n",
    "    X_train_b_final = np.concatenate([X_train_b_num_scaled,\n",
    "                                     X_train_b_cat_encoded.values],\n",
    "                                     axis=1)\n",
    "    X_test_b_final = np.concatenate([X_test_b_num_scaled,\n",
    "                                     X_test_b_cat_encoded],\n",
    "                                     axis=1)\n",
    "\n",
    "    # Encode y\n",
    "    encoder = LabelEncoder()\n",
    "    y_train_b = encoder.fit_transform(y_train_b).copy()\n",
    "    y_test_b = encoder.transform(y_test_b).copy()\n",
    "\n",
    "    # Fit models\n",
    "    log_reg_results_b = do_logistic_r(X_train_b_final, y_train_b)\n",
    "    xgb_classifier_results_b = do_xgb_classifier(X_train_b_final,\n",
    "                                                 y_train_b)\n",
    "    rf_classifier_results_b = do_rf_classifier(X_train_b_final,\n",
    "                                               y_train_b)\n",
    "    \n",
    "    val_results_b = pd.DataFrame({\n",
    "        \"Multinomial Logistic Regression\": [log_reg_results_b[0],\n",
    "                                            log_reg_results_b[1]],\n",
    "        \"XGBoost Classifier\": [xgb_classifier_results_b[0][0],\n",
    "                                xgb_classifier_results_b[0][1]],\n",
    "        \"Random Forest Classifier\": [rf_classifier_results_b[0],\n",
    "                                     rf_classifier_results_b[1]]\n",
    "    })\n",
    "    val_results_b.index = [\"Train Accuracy\", \"Validation Accuracy\"]\n",
    "    val_results_b.to_csv(f\"obj2_results/val_results_3_{target}.csv\")\n",
    "    \n",
    "    return (log_reg_results_b,\n",
    "            xgb_classifier_results_b,\n",
    "            rf_classifier_results_b,\n",
    "            X_test_b_final, y_test_b)\n",
    "\n",
    "val_results_c_opening = third_analysis()\n",
    "val_results_c_closing = third_analysis(\n",
    "    target=\"Deviation direction closing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test results based on best model (XGBoost)\n",
    "best_model_c_opening = val_results_c_opening[1][1]\n",
    "test_predictions_c_opening = best_model_c_opening.predict(\n",
    "    val_results_c_opening[3])\n",
    "test_accuracy_c_opening = accuracy_score(val_results_c_opening[4],\n",
    "                                         test_predictions_c_opening)\n",
    "\n",
    "test_results_c_opening = pd.DataFrame({\n",
    "    \"Test Accuracy XGBoost\": [test_accuracy_c_opening],\n",
    "})\n",
    "test_results_c_opening.to_csv(\"obj2_results/test_results_3_deviation_direction_opening.csv\",\n",
    "                              index=False)\n",
    "\n",
    "# Test results based on best model (XGBoost)\n",
    "best_model_c_closing = val_results_c_closing[1][1]\n",
    "test_predictions_c_closing = best_model_c_closing.predict(\n",
    "    val_results_c_closing[3])\n",
    "test_accuracy_c_closing = accuracy_score(val_results_c_closing[4],\n",
    "                                         test_predictions_c_closing)\n",
    "\n",
    "test_results_c_closing = pd.DataFrame({\n",
    "    \"Test Accuracy XGBoost\": [test_accuracy_c_closing],\n",
    "})\n",
    "test_results_c_closing.to_csv(\"obj2_results/test_results_3_deviation_direction_closing.csv\",\n",
    "                              index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0    624\n",
      "0.1    163\n",
      "0.2    105\n",
      "0.3     44\n",
      "2.2     39\n",
      "2.4     37\n",
      "2.3     30\n",
      "2.7     30\n",
      "2.1     27\n",
      "1.9     23\n",
      "2.5     22\n",
      "2.6     22\n",
      "1.7     21\n",
      "2.8     21\n",
      "2.9     19\n",
      "2.0     18\n",
      "3.1     18\n",
      "3.7     17\n",
      "3.3     16\n",
      "1.8     15\n",
      "3.5     15\n",
      "3.2     13\n",
      "3.6     13\n",
      "0.4     12\n",
      "3.0     12\n",
      "1.3     12\n",
      "3.4     11\n",
      "1.0     10\n",
      "0.8     10\n",
      "0.5      9\n",
      "3.9      9\n",
      "1.5      9\n",
      "3.8      7\n",
      "1.1      7\n",
      "1.2      6\n",
      "1.6      6\n",
      "4.3      6\n",
      "0.9      6\n",
      "0.7      5\n",
      "1.4      5\n",
      "0.6      5\n",
      "4.0      4\n",
      "4.1      3\n",
      "4.6      2\n",
      "4.8      2\n",
      "5.8      1\n",
      "4.2      1\n",
      "4.4      1\n",
      "4.9      1\n",
      "6.4      1\n",
      "Name: Lateral deviation opening, dtype: int64\n",
      "0.0    118\n",
      "0.1     31\n",
      "0.2     19\n",
      "2.2     14\n",
      "2.4      9\n",
      "1.9      9\n",
      "0.3      9\n",
      "2.7      8\n",
      "2.3      7\n",
      "2.6      7\n",
      "2.0      6\n",
      "2.1      6\n",
      "2.8      5\n",
      "1.8      5\n",
      "2.5      4\n",
      "3.1      4\n",
      "2.9      4\n",
      "3.7      4\n",
      "0.4      3\n",
      "1.1      3\n",
      "3.5      3\n",
      "1.3      3\n",
      "3.9      2\n",
      "3.3      2\n",
      "3.2      2\n",
      "1.7      2\n",
      "3.0      2\n",
      "0.9      1\n",
      "4.3      1\n",
      "1.5      1\n",
      "1.6      1\n",
      "4.8      1\n",
      "1.4      1\n",
      "0.5      1\n",
      "3.4      1\n",
      "0.6      1\n",
      "3.6      1\n",
      "3.8      1\n",
      "0.7      1\n",
      "Name: Lateral deviation opening, dtype: int64\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'iloc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32md:\\Tash_files\\For TMD Diagnostics [Task 3]\\Code\\tmd_diagnostics_obj2_1.ipynb Cell 37\u001b[0m line \u001b[0;36m8\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Tash_files/For%20TMD%20Diagnostics%20%5BTask%203%5D/Code/tmd_diagnostics_obj2_1.ipynb#X60sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m     val_results_b\u001b[39m.\u001b[39mto_csv(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mobj2_results/val_results_final_\u001b[39m\u001b[39m{\u001b[39;00mtarget\u001b[39m}\u001b[39;00m\u001b[39m.csv\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Tash_files/For%20TMD%20Diagnostics%20%5BTask%203%5D/Code/tmd_diagnostics_obj2_1.ipynb#X60sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m (lin_reg_results_b,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Tash_files/For%20TMD%20Diagnostics%20%5BTask%203%5D/Code/tmd_diagnostics_obj2_1.ipynb#X60sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m             xgb_regressor_results_b,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Tash_files/For%20TMD%20Diagnostics%20%5BTask%203%5D/Code/tmd_diagnostics_obj2_1.ipynb#X60sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m             rf_regressor_results_b,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Tash_files/For%20TMD%20Diagnostics%20%5BTask%203%5D/Code/tmd_diagnostics_obj2_1.ipynb#X60sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m             X_test_b_final, y_test_b)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Tash_files/For%20TMD%20Diagnostics%20%5BTask%203%5D/Code/tmd_diagnostics_obj2_1.ipynb#X60sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m val_results_final_opening \u001b[39m=\u001b[39m final_analysis()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Tash_files/For%20TMD%20Diagnostics%20%5BTask%203%5D/Code/tmd_diagnostics_obj2_1.ipynb#X60sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m val_results_final_closing \u001b[39m=\u001b[39m final_analysis(\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Tash_files/For%20TMD%20Diagnostics%20%5BTask%203%5D/Code/tmd_diagnostics_obj2_1.ipynb#X60sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m     target\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLateral deviation closing\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32md:\\Tash_files\\For TMD Diagnostics [Task 3]\\Code\\tmd_diagnostics_obj2_1.ipynb Cell 37\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Tash_files/For%20TMD%20Diagnostics%20%5BTask%203%5D/Code/tmd_diagnostics_obj2_1.ipynb#X60sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m X_test_b_final \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate([X_test_b_num_scaled,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Tash_files/For%20TMD%20Diagnostics%20%5BTask%203%5D/Code/tmd_diagnostics_obj2_1.ipynb#X60sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m                                  X_test_b_cat_encoded],\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Tash_files/For%20TMD%20Diagnostics%20%5BTask%203%5D/Code/tmd_diagnostics_obj2_1.ipynb#X60sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m                                  axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Tash_files/For%20TMD%20Diagnostics%20%5BTask%203%5D/Code/tmd_diagnostics_obj2_1.ipynb#X60sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m \u001b[39m# Fit models\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Tash_files/For%20TMD%20Diagnostics%20%5BTask%203%5D/Code/tmd_diagnostics_obj2_1.ipynb#X60sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m lin_reg_results_b \u001b[39m=\u001b[39m do_linear_r(X_train_b_final, y_train_b)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Tash_files/For%20TMD%20Diagnostics%20%5BTask%203%5D/Code/tmd_diagnostics_obj2_1.ipynb#X60sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m xgb_regressor_results_b \u001b[39m=\u001b[39m do_xgb_regressor(X_train_b_final,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Tash_files/For%20TMD%20Diagnostics%20%5BTask%203%5D/Code/tmd_diagnostics_obj2_1.ipynb#X60sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m                                            y_train_b)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Tash_files/For%20TMD%20Diagnostics%20%5BTask%203%5D/Code/tmd_diagnostics_obj2_1.ipynb#X60sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m rf_regressor_results_b \u001b[39m=\u001b[39m do_rf_regressor(X_train_b_final,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Tash_files/For%20TMD%20Diagnostics%20%5BTask%203%5D/Code/tmd_diagnostics_obj2_1.ipynb#X60sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m                                          y_train_b)\n",
      "\u001b[1;32md:\\Tash_files\\For TMD Diagnostics [Task 3]\\Code\\tmd_diagnostics_obj2_1.ipynb Cell 37\u001b[0m line \u001b[0;36m8\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Tash_files/For%20TMD%20Diagnostics%20%5BTask%203%5D/Code/tmd_diagnostics_obj2_1.ipynb#X60sZmlsZQ%3D%3D?line=83'>84</a>\u001b[0m cv_lr \u001b[39m=\u001b[39m KFold(n_splits\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Tash_files/For%20TMD%20Diagnostics%20%5BTask%203%5D/Code/tmd_diagnostics_obj2_1.ipynb#X60sZmlsZQ%3D%3D?line=84'>85</a>\u001b[0m \u001b[39mfor\u001b[39;00m train, val \u001b[39min\u001b[39;00m cv_lr\u001b[39m.\u001b[39msplit(X_train, y_train):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Tash_files/For%20TMD%20Diagnostics%20%5BTask%203%5D/Code/tmd_diagnostics_obj2_1.ipynb#X60sZmlsZQ%3D%3D?line=85'>86</a>\u001b[0m     X_train_temp \u001b[39m=\u001b[39m X_train\u001b[39m.\u001b[39;49miloc[train]\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Tash_files/For%20TMD%20Diagnostics%20%5BTask%203%5D/Code/tmd_diagnostics_obj2_1.ipynb#X60sZmlsZQ%3D%3D?line=86'>87</a>\u001b[0m     X_val_temp \u001b[39m=\u001b[39m X_train\u001b[39m.\u001b[39miloc[val]\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Tash_files/For%20TMD%20Diagnostics%20%5BTask%203%5D/Code/tmd_diagnostics_obj2_1.ipynb#X60sZmlsZQ%3D%3D?line=87'>88</a>\u001b[0m     y_train_temp \u001b[39m=\u001b[39m y_train\u001b[39m.\u001b[39miloc[train]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'iloc'"
     ]
    }
   ],
   "source": [
    "def final_analysis(target = \"Lateral deviation opening\"):\n",
    "    # Form data\n",
    "    second_data = full_data.copy()\n",
    "    second_data = full_data.drop(columns=['Max opening', 'Max closing',\n",
    "                                         'Opening lateral right', 'Opening lateral left',\n",
    "                                         'Closing lateral right', 'Closing lateral left']).copy()\n",
    "\n",
    "    X_b = second_data.drop(columns=[\"Lateral deviation opening\",\n",
    "                                    \"Lateral deviation closing\"]).copy()\n",
    "    y_b_org = second_data[target].copy()\n",
    "\n",
    "    # Get splits\n",
    "    X_train_b, X_test_b, y_train_b, y_test_b = get_splits(\n",
    "        X_b, y_b_org, type=\"regression\"\n",
    "    )\n",
    "\n",
    "    # Scale and create dummies\n",
    "    scaler = StandardScaler()\n",
    "    X_train_b_num = X_train_b.drop(columns=[\"Deviation scenario\",\n",
    "                                            \"Deviation opening\",\n",
    "                                            \"Deviation closing\",\n",
    "                                            \"Deviation direction opening\",\n",
    "                                            \"Deviation direction closing\"\n",
    "                                            ]).copy()\n",
    "    X_train_b_cat = X_train_b[[\"Deviation scenario\",\n",
    "                               \"Deviation opening\",\n",
    "                               \"Deviation closing\",\n",
    "                               \"Deviation direction opening\",\n",
    "                               \"Deviation direction closing\"\n",
    "                               ]].copy()\n",
    "    X_test_b_num = X_test_b.drop(columns=[\"Deviation scenario\",\n",
    "                                          \"Deviation opening\",\n",
    "                                          \"Deviation closing\",\n",
    "                                          \"Deviation direction opening\",\n",
    "                                          \"Deviation direction closing\"\n",
    "                                         ]).copy()\n",
    "    X_test_b_cat = X_test_b[[\"Deviation scenario\",\n",
    "                             \"Deviation opening\",\n",
    "                             \"Deviation closing\",\n",
    "                             \"Deviation direction opening\",\n",
    "                             \"Deviation direction closing\"\n",
    "                            ]].copy()\n",
    "\n",
    "    X_train_b_num_scaled = scaler.fit_transform(X_train_b_num)\n",
    "    X_test_b_num_scaled = scaler.transform(X_test_b_num)\n",
    "    X_train_b_cat_encoded = pd.get_dummies(X_train_b_cat,\n",
    "                                        dtype=int)\n",
    "    X_test_b_cat_encoded = pd.get_dummies(X_test_b_cat,\n",
    "                                        dtype=int)\n",
    "    X_train_b_final = np.concatenate([X_train_b_num_scaled,\n",
    "                                     X_train_b_cat_encoded.values],\n",
    "                                     axis=1)\n",
    "    X_test_b_final = np.concatenate([X_test_b_num_scaled,\n",
    "                                     X_test_b_cat_encoded],\n",
    "                                     axis=1)\n",
    "\n",
    "    # Fit models\n",
    "    lin_reg_results_b = do_linear_r(X_train_b_final, y_train_b)\n",
    "    xgb_regressor_results_b = do_xgb_regressor(X_train_b_final,\n",
    "                                               y_train_b)\n",
    "    rf_regressor_results_b = do_rf_regressor(X_train_b_final,\n",
    "                                             y_train_b)\n",
    "    \n",
    "    val_results_b = pd.DataFrame({\n",
    "        \"Multinomial Logistic Regression\": [lin_reg_results_b[0],\n",
    "                                            lin_reg_results_b[1]],\n",
    "        \"XGBoost Classifier\": [xgb_regressor_results_b[0][0],\n",
    "                                xgb_regressor_results_b[0][1]],\n",
    "        \"Random Forest Classifier\": [rf_regressor_results_b[0],\n",
    "                                     rf_regressor_results_b[1]]\n",
    "    })\n",
    "    val_results_b.index = [\"Train RMSE\", \"Validation RMSE\"]\n",
    "    val_results_b.to_csv(f\"obj2_results/val_results_final_{target}.csv\")\n",
    "    \n",
    "    return (lin_reg_results_b,\n",
    "            xgb_regressor_results_b,\n",
    "            rf_regressor_results_b,\n",
    "            X_test_b_final, y_test_b)\n",
    "\n",
    "val_results_final_opening = final_analysis()\n",
    "val_results_final_closing = final_analysis(\n",
    "    target=\"Lateral deviation closing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test results based on best model (XGBoost)\n",
    "best_model_final_opening = val_results_final_opening[1][1]\n",
    "test_predictions_final_opening = best_model_final_opening.predict(\n",
    "    val_results_final_opening[3])\n",
    "\n",
    "# Performance metrics\n",
    "test_mae_final_opening = mean_absolute_error(val_results_final_opening[4], \n",
    "                                             test_predictions_final_opening)\n",
    "test_rmse_final_opening = np.sqrt(mean_squared_error(\n",
    "    val_results_final_opening[4],\n",
    "    test_predictions_final_opening))\n",
    "\n",
    "test_results_final_opening = pd.DataFrame({\n",
    "    \"Test RMSE XGBoost\": [test_rmse_final_opening],\n",
    "    \"Test MAE XGBoost\": [test_mae_final_opening]\n",
    "})\n",
    "test_results_final_opening.to_csv(\"obj2_results/test_results_final_lateral_deviation_opening.csv\",\n",
    "                                  index=False)\n",
    "\n",
    "# Test results based on best model (XGBoost)\n",
    "best_model_final_closing = val_results_final_closing[1][1]\n",
    "test_predictions_final_closing = best_model_final_closing.predict(\n",
    "    val_results_final_closing[3])\n",
    "\n",
    "# Performance metrics\n",
    "test_mae_final_closing = mean_absolute_error(val_results_final_closing[4], \n",
    "                                             test_predictions_final_closing)\n",
    "test_rmse_final_closing = np.sqrt(mean_squared_error(\n",
    "    val_results_final_closing[4],\n",
    "    test_predictions_final_closing))\n",
    "\n",
    "test_results_final_closing = pd.DataFrame({\n",
    "    \"Test RMSE XGBoost\": [test_rmse_final_closing],\n",
    "    \"Test MAE XGBoost\": [test_mae_final_closing]\n",
    "})\n",
    "test_results_final_closing.to_csv(\"obj2_results/test_results_final_lateral_deviation_closing.csv\",\n",
    "                                  index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
